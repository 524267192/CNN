{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86939b8c",
   "metadata": {},
   "source": [
    "## 1.导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ad074a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b0d4fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n网格搜索(Grid Search)和Optuna，找到模型的最佳超参数组合\\n网格搜索适用于超参数空间较小、离散且较少的情况，而Optuna适用于超参数空间较大、连续或离散且较多的情况\\n下面要做的事情：\\n1.换新的面心值标签，现在数据过拟合，训练集下降但是测试集上升或者波动\\n2.考虑正则化或者假如droput层来防止过拟合\\n3.考虑数据预处理中采用数据标准化，让数据均匀分布\\n4.用不用考虑损失函数，学习率,epoch,adam优化器及其四个参数，的修改，模型用不用再添加几层让模型变复杂些（batch-size越大，训练越快，不影响准确率）\\n5.早停法（Early Stopping）：在训练过程中监控验证集上的性能，一旦性能停止改善，在一定epoch后停止训练，并保存模型，以防止过拟合。可以参照外国那案例\\n6.数据集的比例，不一定4：1，也可以95：5，当数据集足够大时，这样可以增加训练集数量\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "网格搜索(Grid Search)和Optuna，找到模型的最佳超参数组合\n",
    "网格搜索适用于超参数空间较小、离散且较少的情况，而Optuna适用于超参数空间较大、连续或离散且较多的情况\n",
    "下面要做的事情：\n",
    "1.换新的面心值标签，现在数据过拟合，训练集下降但是测试集上升或者波动\n",
    "2.考虑正则化或者假如droput层来防止过拟合\n",
    "3.考虑数据预处理中采用数据标准化，让数据均匀分布\n",
    "4.用不用考虑损失函数，学习率,epoch,adam优化器及其四个参数，的修改，模型用不用再添加几层让模型变复杂些（batch-size越大，训练越快，不影响准确率）\n",
    "5.早停法（Early Stopping）：在训练过程中监控验证集上的性能，一旦性能停止改善，在一定epoch后停止训练，并保存模型，以防止过拟合。可以参照外国那案例\n",
    "6.数据集的比例，不一定4：1，也可以95：5，当数据集足够大时，这样可以增加训练集数量\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c88f13",
   "metadata": {},
   "source": [
    "## 2.加载数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4134756",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "1.功能：\n",
    "通过加载data和label文件，然后继续训练和预测。\n",
    "定义了一个6层卷积神经网络模型。每个卷积层后面跟着一个 ReLU 激活函数。第七层只有卷积，没有relu。\n",
    "输入数据n*64*64*2,这里的一个样本64*64可以看成一个图片格式（在此次任务中是速度，两者类似）\n",
    "输出是n*64*64*4\n",
    "\"\"\"\n",
    "\"\"\"txt保存为numpy格式发现可以减少存储大小，约缩小成1/4\n",
    "5.9G\t./all_data.npy\n",
    "12G\t./all_label.npy\n",
    "27G\t./data_64x64x2.txt\n",
    "53G\t./label_a_2x64x65x2.txt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bea48b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接加载npy文件为numpy格式\n",
    "all_data = np.load('./data/all_data.npy')\n",
    "# #直接加载npy文件为numpy格式,注意标签是面心值，不是a值\n",
    "all_label = np.load('./data/all_centerFace_label.npy')\n",
    "\n",
    "all_data = torch.tensor(all_data).float()\n",
    "all_label = torch.tensor(all_label).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cedeff1",
   "metadata": {},
   "source": [
    "## 3.构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31a2c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#神经网络模型\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_output_channels, dropout):#加了 dropout\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=dropout)  # 添加dropout层\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=dropout)  # 添加dropout层\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(p=dropout)  # 添加dropout层\n",
    "\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout4 = nn.Dropout(p=dropout)  # 添加dropout层\n",
    "\n",
    "        self.conv5 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.dropout5 = nn.Dropout(p=dropout)  # 添加dropout层\n",
    "\n",
    "        self.conv6 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.dropout6 = nn.Dropout(p=dropout)  # 添加dropout层\n",
    "\n",
    "        self.conv7 = nn.Conv2d(64, num_output_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)  # dropout层应用于卷积层之后\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.dropout5(x)\n",
    "\n",
    "        x = self.conv6(x)\n",
    "        x = self.relu6(x)\n",
    "        x = self.dropout6(x)\n",
    "\n",
    "        x = self.conv7(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7726cbea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n早停法是一种被广泛使用的方法，在很多案例上都比正则化的方法要好。\\n其基本含义是在训练中计算模型在验证集上的表现，\\n当模型在验证集上的表现开始下降的时候，停止训练，这样就能避免继续训练导致过拟合的问题。\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "早停法是一种被广泛使用的方法，在很多案例上都比正则化的方法要好。\n",
    "其基本含义是在训练中计算模型在验证集上的表现，\n",
    "当模型在验证集上的表现开始下降的时候，停止训练，这样就能避免继续训练导致过拟合的问题。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962aa34b",
   "metadata": {},
   "source": [
    "## 4.模型训练与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4249cbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "采用DataParallel加速，device_count个数为： 4\n",
      "begin to train!!!\n",
      "已完成第： 1 个epoch! Train Loss: 1348.7728939056396 Test Loss: 321.59468483924866\n",
      "已完成第： 2 个epoch! Train Loss: 1260.4784739017487 Test Loss: 310.48903608322144\n",
      "已完成第： 3 个epoch! Train Loss: 1234.4498298168182 Test Loss: 307.27298736572266\n",
      "已完成第： 4 个epoch! Train Loss: 1224.1986955404282 Test Loss: 305.1206524372101\n",
      "已完成第： 5 个epoch! Train Loss: 1216.5939611196518 Test Loss: 303.2509905099869\n",
      "已完成第： 6 个epoch! Train Loss: 1210.4738811254501 Test Loss: 301.6738655567169\n",
      "已完成第： 7 个epoch! Train Loss: 1204.7675420045853 Test Loss: 300.42883360385895\n",
      "已完成第： 8 个epoch! Train Loss: 1199.8888589143753 Test Loss: 299.85322654247284\n",
      "已完成第： 9 个epoch! Train Loss: 1196.0664948225021 Test Loss: 298.9837063550949\n",
      "已完成第： 10 个epoch! Train Loss: 1192.4784274101257 Test Loss: 297.69097673892975\n",
      "已完成第： 11 个epoch! Train Loss: 1188.432923078537 Test Loss: 297.32847332954407\n",
      "已完成第： 12 个epoch! Train Loss: 1185.208750963211 Test Loss: 296.2691606283188\n",
      "已完成第： 13 个epoch! Train Loss: 1182.867956995964 Test Loss: 296.1202372312546\n",
      "已完成第： 14 个epoch! Train Loss: 1179.934931397438 Test Loss: 294.259761929512\n",
      "已完成第： 15 个epoch! Train Loss: 1177.5216228961945 Test Loss: 294.5364283323288\n",
      "已完成第： 16 个epoch! Train Loss: 1175.318153142929 Test Loss: 294.1038910150528\n",
      "已完成第： 17 个epoch! Train Loss: 1173.279995918274 Test Loss: 292.795618057251\n",
      "已完成第： 18 个epoch! Train Loss: 1171.2049568891525 Test Loss: 292.47431552410126\n",
      "已完成第： 19 个epoch! Train Loss: 1170.3834739923477 Test Loss: 292.14828073978424\n",
      "已完成第： 20 个epoch! Train Loss: 1167.3688068389893 Test Loss: 291.62877082824707\n",
      "已完成第： 21 个epoch! Train Loss: 1166.5996780395508 Test Loss: 291.3276046514511\n",
      "已完成第： 22 个epoch! Train Loss: 1164.7690780162811 Test Loss: 290.9938942193985\n",
      "已完成第： 23 个epoch! Train Loss: 1163.3292450904846 Test Loss: 291.5942611694336\n",
      "已完成第： 24 个epoch! Train Loss: 1161.973576426506 Test Loss: 290.25706720352173\n",
      "已完成第： 25 个epoch! Train Loss: 1160.4698063135147 Test Loss: 290.1233205795288\n",
      "已完成第： 26 个epoch! Train Loss: 1159.3253104686737 Test Loss: 289.5267473459244\n",
      "已完成第： 27 个epoch! Train Loss: 1158.4068396091461 Test Loss: 290.13163661956787\n",
      "已完成第： 28 个epoch! Train Loss: 1157.4005962610245 Test Loss: 289.06031918525696\n",
      "已完成第： 29 个epoch! Train Loss: 1155.9755644798279 Test Loss: 290.3972442150116\n",
      "已完成第： 30 个epoch! Train Loss: 1155.7193430662155 Test Loss: 288.7264355421066\n",
      "已完成第： 31 个epoch! Train Loss: 1154.1200469732285 Test Loss: 288.87189495563507\n",
      "已完成第： 32 个epoch! Train Loss: 1153.8005117177963 Test Loss: 288.31063520908356\n",
      "已完成第： 33 个epoch! Train Loss: 1152.8338739871979 Test Loss: 288.20392894744873\n",
      "已完成第： 34 个epoch! Train Loss: 1152.262924671173 Test Loss: 288.1924102306366\n",
      "已完成第： 35 个epoch! Train Loss: 1151.4694381952286 Test Loss: 288.1795370578766\n",
      "已完成第： 36 个epoch! Train Loss: 1151.1398259401321 Test Loss: 287.69027507305145\n",
      "已完成第： 37 个epoch! Train Loss: 1150.4336013793945 Test Loss: 287.44134771823883\n",
      "已完成第： 38 个epoch! Train Loss: 1149.5472028255463 Test Loss: 287.5704081058502\n",
      "已完成第： 39 个epoch! Train Loss: 1148.9662820100784 Test Loss: 287.97021746635437\n",
      "已完成第： 40 个epoch! Train Loss: 1148.205924630165 Test Loss: 287.4354922771454\n",
      "已完成第： 41 个epoch! Train Loss: 1147.7674651145935 Test Loss: 287.0742394924164\n",
      "已完成第： 42 个epoch! Train Loss: 1147.0148367881775 Test Loss: 287.2793537378311\n",
      "已完成第： 43 个epoch! Train Loss: 1146.4981533288956 Test Loss: 287.23376619815826\n",
      "已完成第： 44 个epoch! Train Loss: 1146.381409406662 Test Loss: 287.44508123397827\n",
      "已完成第： 45 个epoch! Train Loss: 1145.9969288110733 Test Loss: 286.44932794570923\n",
      "已完成第： 46 个epoch! Train Loss: 1145.2137962579727 Test Loss: 287.01486337184906\n",
      "已完成第： 47 个epoch! Train Loss: 1144.9991199970245 Test Loss: 286.5883551836014\n",
      "已完成第： 48 个epoch! Train Loss: 1144.5471453666687 Test Loss: 286.2310165166855\n",
      "已完成第： 49 个epoch! Train Loss: 1144.2274038791656 Test Loss: 286.82838094234467\n",
      "已完成第： 50 个epoch! Train Loss: 1143.3362282514572 Test Loss: 286.10381186008453\n",
      "已完成第： 51 个epoch! Train Loss: 1143.2487332820892 Test Loss: 285.96377539634705\n",
      "已完成第： 52 个epoch! Train Loss: 1142.641880273819 Test Loss: 285.8446806669235\n",
      "已完成第： 53 个epoch! Train Loss: 1142.6164292097092 Test Loss: 286.07012808322906\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 108\u001b[0m\n\u001b[1;32m    104\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(net, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./model/model_origin_500_optuna_01.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_loss, test_loss\n\u001b[0;32m--> 108\u001b[0m train_loss, test_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_output_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_loss)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_loss)\n",
      "Cell \u001b[0;32mIn[6], line 70\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_output_channels)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# 测试\u001b[39;00m\n\u001b[1;32m     69\u001b[0m running_test_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(testloader):\n\u001b[1;32m     71\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     72\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m net(inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    560\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    563\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:47\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:84\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     83\u001b[0m     transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:84\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     83\u001b[0m     transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:56\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     54\u001b[0m         storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_new_shared(numel)\n\u001b[1;32m     55\u001b[0m         out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr_\u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring_\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndarray\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemmap\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;66;03m# array of string classes and object\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train(num_output_channels):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 定义一个batch包含的样本数目\n",
    "    batch_size = 256\n",
    "\n",
    "    # 生成数据集\n",
    "    x_train, x_test, y_train, y_test = train_test_split(all_data, all_label, test_size=0.2)\n",
    "\n",
    "    # 设置种子数\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # 划分数据集\n",
    "    trainset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 划分数据集\n",
    "    testset = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 创建模型实例，并将模型移动到GPU设备上进行计算\n",
    "    net = Net(num_output_channels,0.177668).to(device)\n",
    "    \n",
    "    # 加速训练：如果有多个GPU，则使用DataParallel模块\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        net = nn.DataParallel(net)\n",
    "        print(\"采用DataParallel加速，device_count个数为：\", str(torch.cuda.device_count()))\n",
    "        \n",
    "    # 定义损失函数为均方误差\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # 定义优化器为Adam优化器，设置学习率为0.001\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.000725)\n",
    "\n",
    "    print('begin to train!!!')\n",
    "\n",
    "    # 训练模型\n",
    "    num_epochs = 500  # 训练轮数\n",
    "    \n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    best_loss = float('inf')  # 初始化最佳验证集损失值为正无穷大\n",
    "    patience = 12  # 设置连续多少次验证集损失值不下降时停止训练\n",
    "    count = 0  # 记录连续不下降次数\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        #训练\n",
    "        running_train_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(trainloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "            \n",
    "        train_loss.append(running_train_loss)\n",
    "        \n",
    "        # 测试\n",
    "        running_test_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(testloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_test_loss += loss.item()\n",
    "            \n",
    "        test_loss.append(running_test_loss)\n",
    "        print(\"已完成第：\", str(epoch+1), \"个epoch! Train Loss:\", running_train_loss, \"Test Loss:\", running_test_loss)\n",
    "\n",
    "        # 早停法\n",
    "        \"\"\"\n",
    "        如果<连续多个 epoch> 的<验证集损失值>都没有<下降>，即验证集损失值不再降低，那么就会认为模型已经过拟合或者无法继续改善。\n",
    "        这时，训练会提前停止，并保存当前的模型参数\n",
    "        \"\"\"\n",
    "        if running_test_loss < best_loss:\n",
    "\n",
    "            best_loss = running_test_loss\n",
    "            count = 0 #连续十次测试集的epoch loss不下降，故只要又一次下降，就清零重新计算\n",
    "        else:\n",
    "            count += 1\n",
    "            if count >= patience:\n",
    "                print(f\"验证集损失值连续{patience}次不下降，停止训练！\")\n",
    "                break\n",
    "                \n",
    "#         if epoch==500:\n",
    "#             torch.save(net, './model/model_origin_500_01.pth')\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    process_time = end_time - start_time\n",
    "    print(f\"模型训练和测试共用了: {process_time} 秒！\")\n",
    "    print('all of tasks Finished')\n",
    "    \n",
    "    # 保存整个模型\n",
    "    torch.save(net, './model/model_origin_500_optuna_01.pth')\n",
    "    \n",
    "    return train_loss, test_loss\n",
    "\n",
    "train_loss, test_loss = train(num_output_channels=4)\n",
    "print(train_loss)\n",
    "print(test_loss)\n",
    "np.save('./data/lossa/loss_model_500_optuna_01.npy', np.array([train_loss, test_loss]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eef3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_data.shape)\n",
    "print(all_label.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c768c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss,test_loss = np.load('./data/lossa/loss_model_500_optuna_01.npy')\n",
    "print(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e124e0",
   "metadata": {},
   "source": [
    "# 5.可视化loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff7cdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 创建 x 轴数据，假设为 epoch 数\n",
    "epoch = np.arange(1, len(train_loss) + 1)\n",
    "\n",
    "train_loss = [round(int(a)/(all_data.shape[0]*0.8), 5) for a in train_loss]\n",
    "print(train_loss)\n",
    "# 绘制训练损失曲线\n",
    "plt.plot(epoch, train_loss, 'b', label='Train Loss')\n",
    "\n",
    "# 关闭科学计数法\n",
    "plt.gca().get_yaxis().get_major_formatter().set_scientific(False)\n",
    "\n",
    "# 设置y轴范围下限为0\n",
    "# plt.ylim(bottom=0)\n",
    "\n",
    "# 设置图表标题和轴标签\n",
    "plt.title('Train Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# 添加图例\n",
    "plt.legend()\n",
    "\n",
    "# 显示图表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ebd587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 x 轴数据，假设为 epoch 数\n",
    "epoch = np.arange(1, len(test_loss) + 1)\n",
    "print(test_loss)\n",
    "test_loss = [round(int(a)/(all_data.shape[0]*0.2), 5) for a in test_loss]\n",
    "\n",
    "\n",
    "# 绘制测试损失曲线\n",
    "plt.plot(epoch, test_loss, 'r', label='Test Loss')\n",
    "\n",
    "# 设置图表标题和轴标签\n",
    "plt.title('Test Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# 添加图例\n",
    "plt.legend()\n",
    "\n",
    "# 显示图表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bec1e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# # 将模型移动到适当的设备\n",
    "# model = model.to(device)\n",
    "\n",
    "# # 打印模型的概要信息\n",
    "# summary(model,input_size=(2, 64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ce42f0",
   "metadata": {},
   "source": [
    "## 6.预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c8b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载模型\n",
    "model = torch.load('./model/model_origin_500_optuna_01.pth', map_location=device)# from torchsummary import summary\n",
    "# # 将模型移动到适当的设备\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546182fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 输入数据进行预测\n",
    "input_data =all_data[-10]  # 你的输入数据\n",
    "\n",
    "\n",
    "#调整输入input的维度顺序,作为E，用于下面change_Label_to_a中(E-A)/(B-A)得到a值\n",
    "matrix_64 = input_data.cpu()\n",
    "matrix_64 = matrix_64.permute(1,2,0)  \n",
    "print(matrix_64.shape)\n",
    "\n",
    "\n",
    "# 转换为四维\n",
    "input_data = input_data.unsqueeze(0)#用实际数据，数据格式为(1,2, 64, 64)，不能为2x64x64\n",
    "# input_data = torch.randn(1,2, 64, 64)\n",
    "print(input_data.shape)\n",
    "input_tensor = input_data.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)##如果报错的话需要把网络的设计加上，里面涉及model(input)\n",
    "\n",
    "# 打印预测结果\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830e32fe",
   "metadata": {},
   "source": [
    "# 8.转换格式(label转为最终的weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d94569",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将label面心值转为a值\n",
    "def change_Label_to_a(all_vertical_edge_centers,all_horizontal_edge_centers):\n",
    "    a_vertical = np.zeros((64, 65, 2))\n",
    "    # a_vertical = np.random((64, 65, 2))\n",
    "    a_horizontal = np.zeros((64, 65, 2))\n",
    "    # a_horizontal = np.random.random((64, 65, 2))\n",
    "\n",
    "\n",
    "    # 21. 求a:   横着的边，分两种情况，边缘（对称的）和非边缘的边.这里matrix_64要行列互换，因为横着时面心值是一列一列求得，竖着时是一行一行求的。\n",
    "    for i in range(64):\n",
    "        for j in range(65):\n",
    "            if j == 0 or j == 64:\n",
    "                # a_horizontal[i, j] = (all_horizontal_edge_centers[i, 0] - matrix_64[i, 0]) / (matrix_64[i, 63] - matrix_64[i, 0])#换之前\n",
    "                a_horizontal[i, j] = (all_horizontal_edge_centers[i, 0] - matrix_64[0, i]) / (\n",
    "                    matrix_64[63, i] - matrix_64[0, i])\n",
    "            else:\n",
    "                # aA=(1-a)B=E   a = (E-B)/(A-B) 其中：A为matrix_64[i,j]，B为matrix_64[i,j+1]\n",
    "                # a_horizontal[i, j] = (all_horizontal_edge_centers[i, j] - matrix_64[i, j]) / (matrix_64[i, j-1] - matrix_64[i, j])\n",
    "                a_horizontal[i, j] = (all_horizontal_edge_centers[i, j] - matrix_64[j, i]) / (\n",
    "                    matrix_64[j - 1, i] - matrix_64[j, i])\n",
    "\n",
    "    # 22. 求a:   竖着的边，分两种情况，边缘（对称的）和非边缘的边\n",
    "    for i in range(64):\n",
    "        for j in range(65):\n",
    "            if j == 0 or j == 64:\n",
    "                a_vertical[i, j] = (all_vertical_edge_centers[i, 0] - matrix_64[i, 0]) / (\n",
    "                    matrix_64[i, 63] - matrix_64[i, 0])\n",
    "            else:\n",
    "                # aA=(1-a)B=E   a = (E-B)/(A-B) 其中：A为matrix_64[i,j]，B为matrix_64[i,j+1]\n",
    "                # 2.错误matrix_64[i, j-1]) / (改成matrix_64[i, j]) / (\n",
    "                a_vertical[i, j] = (all_vertical_edge_centers[i, j] - matrix_64[i, j]) / (\n",
    "                    matrix_64[i, j - 1] - matrix_64[i, j])\n",
    "\n",
    "    # # 若最终a对应的矩阵里面出现无穷，则将其替换为0.5.解决了分母为0的问题\n",
    "    # a_vertical[np.isinf(a_vertical)] = 0.5\n",
    "    # a_horizontal[np.isinf(a_horizontal)] = 0.5\n",
    "    #这里64x65x2截成64x64x2,因为边框对称时值相同\n",
    "    \n",
    "    a_vertical = torch.tensor(a_vertical[:,:64,:])\n",
    "    a_horizontal = torch.tensor(a_horizontal[:,:64,:])\n",
    "\n",
    "    print(a_vertical.shape)\n",
    "    print(a_horizontal.shape)\n",
    "    return  a_vertical,a_horizontal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc96529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#返回一个列表，里面嵌套两个子列表，第一个子列表存放的是内部的a值，第二个子列表存放的是边框的a值，\n",
    "#且顺序为上（左到右），下（左到右），左（下到上），右（下到上）\n",
    "def conversion_format(output):\n",
    "    # 调整output的维度顺序\n",
    "    output = output.permute(0,2,3,1)  \n",
    "    print(output.shape)\n",
    "    output = output[0]\n",
    "    print(output.shape)\n",
    "    # print(output)\n",
    "\n",
    "    #将输出output拆成两个面心值\n",
    "    all_vertical_edge_centers = output[:, :, 0:2].cpu()\n",
    "    all_horizontal_edge_centers = output[:, :, 2:4].cpu()\n",
    "    print(all_vertical_edge_centers.shape)\n",
    "    print(all_horizontal_edge_centers.shape)\n",
    "\n",
    "    \n",
    "    a_vertical,a_horizontal  = change_Label_to_a(all_vertical_edge_centers,all_horizontal_edge_centers)    \n",
    "    \n",
    "    # 将前两个元素相加除以二得到一个元素(x+y/)2\n",
    "    avg_vertical = (a_vertical[:, :, 0] + a_vertical[:, :, 1]) / 2\n",
    "    # 将后两个元素相加除以二得到另一个元素\n",
    "    avg_horizontal = (a_horizontal[:, :, 0] + a_horizontal[:, :, 1]) / 2\n",
    "\n",
    "    # 重新组合成新的形状为(64, 64, 2)的张量\n",
    "    new_avg_a_output = torch.stack([avg_vertical, avg_horizontal], dim=2)\n",
    "    # 打印转换后的数据形状\n",
    "    print(new_avg_a_output.shape)\n",
    "\n",
    "    #返回两个求完平均的面心值,包括两个64*64矩阵，矩阵是求完平均后的a值，一个竖着的，一个横着的\n",
    "    vertical_1d = new_avg_a_output[:, :, 0]\n",
    "    horizontal_1d = new_avg_a_output[:, :,1]\n",
    "    print(vertical_1d.shape)\n",
    "    print(horizontal_1d.shape)\n",
    "    \n",
    "    \n",
    "    border = []#存所有边框，四个边框\n",
    "    left_border=[]#存左边框\n",
    "    bottom_border = []#存下边框\n",
    "    inner = []#存内部的面心值\n",
    "\n",
    "    #下面将2个64x64面心值变换格式，返回指定的格式result\n",
    "    for i in range(len(vertical_1d)): #两个for循环等价于for i in range(64):\n",
    "        for j in range(len(vertical_1d[i])):\n",
    "            if j ==0:#j=0添加边框\n",
    "                #添加左边框\n",
    "                left_border.append(vertical_1d[i][0])\n",
    "                #添加下边框\n",
    "                bottom_border.append(horizontal_1d[i][0])\n",
    "            else: \n",
    "                if i !=  63:#当竖着的最后一行时，上面没有对应的横着的\n",
    "                    inner.append(vertical_1d[i][j])#竖着的\n",
    "    #                 print(\"{j-1},{i+1}\",j-1,i+1)\n",
    "                    inner.append(horizontal_1d[j-1][i+1])   #再横着的   \n",
    "                    if j == 63:#如果j=63的话，还需要再加入最后一列的横着的边\n",
    "                        inner.append(horizontal_1d[63][i+1])#当i=63,横着的加最后一列 \n",
    "                else:#if i ==63 :\n",
    "                    inner.append(vertical_1d[63][j])#当i=63,inner最后添加竖着的一行竖线\n",
    "\n",
    "    inner = [tensor.cpu().numpy().tolist() for tensor in inner]#将一维列表里面的tensor元素转为numpy格式，并返回cpu版本\n",
    "    # print(inner)\n",
    "    four_border = [bottom_border,bottom_border,left_border,left_border]#顺序是上（左到右），下（左到右），左（下到上），右（下到上）\n",
    "    border = [item.numpy().tolist() for sublist in four_border for item in sublist]\n",
    "    result = [inner,border]\n",
    "    print(len(result))\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5227f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "result  = conversion_format(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fa4c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(result[0]))\n",
    "print(len(result[1]))\n",
    "print(8064+64*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce8e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = sum(1 for num in result[0] if 0 < num < 1)\n",
    "print(count)\n",
    "print(count/len(result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b1d97b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9b6dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b244dce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9397acc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
