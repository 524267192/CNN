{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86939b8c",
   "metadata": {},
   "source": [
    "## 1.导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ad074a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b0d4fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n网格搜索(Grid Search)和Optuna，找到模型的最佳超参数组合\\n网格搜索适用于超参数空间较小、离散且较少的情况，而Optuna适用于超参数空间较大、连续或离散且较多的情况\\n下面要做的事情：\\n1.换新的面心值标签，现在数据过拟合，训练集下降但是测试集上升或者波动\\n2.考虑正则化或者假如droput层来防止过拟合\\n3.考虑数据预处理中采用数据标准化，让数据均匀分布\\n4.用不用考虑损失函数，学习率,epoch,adam优化器及其四个参数，的修改，模型用不用再添加几层让模型变复杂些（batch-size越大，训练越快，不影响准确率）\\n5.早停法（Early Stopping）：在训练过程中监控验证集上的性能，一旦性能停止改善，在一定epoch后停止训练，并保存模型，以防止过拟合。可以参照外国那案例\\n6.数据集的比例，不一定4：1，也可以95：5，当数据集足够大时，这样可以增加训练集数量\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "网格搜索(Grid Search)和Optuna，找到模型的最佳超参数组合\n",
    "网格搜索适用于超参数空间较小、离散且较少的情况，而Optuna适用于超参数空间较大、连续或离散且较多的情况\n",
    "下面要做的事情：\n",
    "1.换新的面心值标签，现在数据过拟合，训练集下降但是测试集上升或者波动\n",
    "2.考虑正则化或者假如droput层来防止过拟合\n",
    "3.考虑数据预处理中采用数据标准化，让数据均匀分布\n",
    "4.用不用考虑损失函数，学习率,epoch,adam优化器及其四个参数，的修改，模型用不用再添加几层让模型变复杂些（batch-size越大，训练越快，不影响准确率）\n",
    "5.早停法（Early Stopping）：在训练过程中监控验证集上的性能，一旦性能停止改善，在一定epoch后停止训练，并保存模型，以防止过拟合。可以参照外国那案例\n",
    "6.数据集的比例，不一定4：1，也可以95：5，当数据集足够大时，这样可以增加训练集数量\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c88f13",
   "metadata": {},
   "source": [
    "## 2.加载数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4134756",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "1.功能：\n",
    "通过加载data和label文件，然后继续训练和预测。\n",
    "定义了一个6层卷积神经网络模型。每个卷积层后面跟着一个 ReLU 激活函数。第七层只有卷积，没有relu。\n",
    "输入数据n*64*64*2,这里的一个样本64*64可以看成一个图片格式（在此次任务中是速度，两者类似）\n",
    "输出是n*64*64*4\n",
    "\"\"\"\n",
    "\"\"\"txt保存为numpy格式发现可以减少存储大小，约缩小成1/4\n",
    "5.9G\t./all_data.npy\n",
    "12G\t./all_label.npy\n",
    "27G\t./data_64x64x2.txt\n",
    "53G\t./label_a_2x64x65x2.txt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bea48b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接加载npy文件为numpy格式\n",
    "all_data = np.load('./data/all_data.npy')\n",
    "# #直接加载npy文件为numpy格式,注意标签是面心值，不是a值\n",
    "all_label = np.load('./data/all_centerFace_label.npy')\n",
    "\n",
    "all_data = torch.tensor(all_data).float()\n",
    "all_label = torch.tensor(all_label).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dce8fd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mall_data\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      2\u001b[0m all_label\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_data' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cedeff1",
   "metadata": {},
   "source": [
    "## 3.构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31a2c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#神经网络模型\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_output_channels, dropout):#加了 dropout\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=dropout)  # 添加dropout层\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=dropout)  # 添加dropout层\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(p=dropout)  # 添加dropout层\n",
    "\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout4 = nn.Dropout(p=dropout)  # 添加dropout层\n",
    "\n",
    "        self.conv5 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.dropout5 = nn.Dropout(p=dropout)  # 添加dropout层\n",
    "\n",
    "        self.conv6 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.dropout6 = nn.Dropout(p=dropout)  # 添加dropout层\n",
    "\n",
    "        self.conv7 = nn.Conv2d(64, num_output_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)  # dropout层应用于卷积层之后\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.dropout5(x)\n",
    "\n",
    "        x = self.conv6(x)\n",
    "        x = self.relu6(x)\n",
    "        x = self.dropout6(x)\n",
    "\n",
    "        x = self.conv7(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7726cbea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n早停法是一种被广泛使用的方法，在很多案例上都比正则化的方法要好。\\n其基本含义是在训练中计算模型在验证集上的表现，\\n当模型在验证集上的表现开始下降的时候，停止训练，这样就能避免继续训练导致过拟合的问题。\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "早停法是一种被广泛使用的方法，在很多案例上都比正则化的方法要好。\n",
    "其基本含义是在训练中计算模型在验证集上的表现，\n",
    "当模型在验证集上的表现开始下降的时候，停止训练，这样就能避免继续训练导致过拟合的问题。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59968800",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = './model/model_origin_1000_zaoting_02.pth'\n",
    "save_loss_path = './data/lossa/loss_model_1000_zaoting_02.npy'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962aa34b",
   "metadata": {},
   "source": [
    "## 4.模型训练与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4249cbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import time\n",
    "def objective(trial):\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n",
    "\n",
    "    num_output_channels = 4\n",
    "    # 生成数据集\n",
    "    x_train, x_test, y_train, y_test = train_test_split(all_data, all_label, test_size=0.2)\n",
    "\n",
    "    # 设置种子数\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # 划分数据集\n",
    "    trainset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 划分数据集\n",
    "    testset = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 创建模型实例，并将模型移动到GPU设备上进行计算\n",
    "    net = Net(num_output_channels,dropout).to(device)\n",
    "\n",
    "    # 加速训练：如果有多个GPU，则使用DataParallel模块\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        net = nn.DataParallel(net)\n",
    "        print(\"采用DataParallel加速，device_count个数为：\",str(torch.cuda.device_count()))\n",
    "\n",
    "    # 定义损失函数为均方误差\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # 获取要调优的超参数值\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    optimizer = getattr(optim, optimizer_name)(net.parameters(), lr=lr)\n",
    "        \n",
    "#     定义优化器为Adam优化器\n",
    "#     optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    # 训练模型\n",
    "    num_epochs = 20\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    \n",
    "    best_loss = float('inf')  # 初始化最佳验证集损失值为正无穷大\n",
    "    patience = 10  # 设置连续多少次验证集损失值不下降时停止训练\n",
    "    count = 0  # 记录连续不下降次数\n",
    "    \n",
    "    start_time =  time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_train_loss = 0.0\n",
    "        net.train()#加了这个\n",
    "        for i, (inputs, labels) in enumerate(trainloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "        train_loss.append(running_train_loss)\n",
    "        \n",
    "        running_test_loss = 0.0\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, labels) in enumerate(testloader):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_test_loss += loss.item()\n",
    "        \n",
    "            \n",
    "            test_loss.append(running_test_loss)\n",
    "            print(\"已完成第：\", str(epoch+1), \"个epoch! Train Loss:\", running_train_loss, \"Test Loss:\", running_test_loss)\n",
    "\n",
    "            # 早停法\n",
    "            \"\"\"\n",
    "            如果<连续多个 epoch> 的<验证集损失值>都没有<下降>，即验证集损失值不再降低，那么就会认为模型已经过拟合或者无法继续改善。\n",
    "            这时，训练会提前停止，并保存当前的模型参数\n",
    "            \"\"\"\n",
    "            if running_test_loss < best_loss:\n",
    "\n",
    "                best_loss = running_test_loss\n",
    "                count = 0 #连续十次测试集的epoch loss不下降，故只要又一次下降，就清零重新计算\n",
    "            else:\n",
    "                count += 1\n",
    "                if count >= patience:\n",
    "                    print(f\"验证集损失值连续{patience}次不下降，停止训练！\")\n",
    "                    break\n",
    "\n",
    "#             if epoch==500:\n",
    "#                 torch.save(net, './model/model_origin_500_02.pth')\n",
    "    #     注意\n",
    "    #缺少保存loss代码\n",
    "    #急停法和optuna能不能同时用，看那个外国视频代码\n",
    "    \n",
    "    end_time = time.time()\n",
    "    process_time = end_time - start_time\n",
    "    print(f\"模型训练和测试共用了: {process_time} 秒！\")\n",
    "    print('all of tasks Finished')\n",
    "    \n",
    "    # 保存整个模型\n",
    "    torch.save(net, save_model_path)\n",
    "    \n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # 返回验证集上的最终损失作为目标值\n",
    "    return running_test_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1749aa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-20 17:06:37,386] A new study created in memory with name: no-name-5a080d2c-deef-42c1-b490-895fae9672fd\n",
      "/tmp/ipykernel_3765038/3893867180.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "采用DataParallel加速，device_count个数为： 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3765038/3893867180.py:37: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已完成第： 1 个epoch! Train Loss: 1392.8881187438965 Test Loss: 339.3895390033722\n",
      "已完成第： 2 个epoch! Train Loss: 1324.8325445652008 Test Loss: 328.97430324554443\n",
      "已完成第： 3 个epoch! Train Loss: 1294.918596982956 Test Loss: 318.3123745918274\n",
      "已完成第： 4 个epoch! Train Loss: 1268.1907320022583 Test Loss: 309.579740524292\n",
      "已完成第： 5 个epoch! Train Loss: 1253.023736000061 Test Loss: 305.989667057991\n",
      "已完成第： 6 个epoch! Train Loss: 1244.4158878326416 Test Loss: 303.2557336091995\n",
      "已完成第： 7 个epoch! Train Loss: 1238.0536451339722 Test Loss: 301.8176211118698\n",
      "已完成第： 8 个epoch! Train Loss: 1233.2023161649704 Test Loss: 300.29429483413696\n",
      "已完成第： 9 个epoch! Train Loss: 1228.9670069217682 Test Loss: 299.8783633708954\n",
      "已完成第： 10 个epoch! Train Loss: 1225.885414838791 Test Loss: 298.769523024559\n",
      "已完成第： 11 个epoch! Train Loss: 1222.875124335289 Test Loss: 296.97609436511993\n",
      "已完成第： 12 个epoch! Train Loss: 1220.5153963565826 Test Loss: 296.30419397354126\n",
      "已完成第： 13 个epoch! Train Loss: 1217.7758724689484 Test Loss: 295.4649648666382\n",
      "已完成第： 14 个epoch! Train Loss: 1215.6938894987106 Test Loss: 295.9358859062195\n",
      "已完成第： 15 个epoch! Train Loss: 1213.8129489421844 Test Loss: 294.61317467689514\n",
      "已完成第： 16 个epoch! Train Loss: 1212.3750451803207 Test Loss: 294.50588488578796\n",
      "已完成第： 17 个epoch! Train Loss: 1210.9217482805252 Test Loss: 293.151091337204\n",
      "已完成第： 18 个epoch! Train Loss: 1209.2617832422256 Test Loss: 292.7108346223831\n",
      "已完成第： 19 个epoch! Train Loss: 1207.934569478035 Test Loss: 292.5029183626175\n",
      "已完成第： 20 个epoch! Train Loss: 1206.9691410064697 Test Loss: 291.7751262187958\n",
      "模型训练和测试共用了: 811.0142769813538 秒！\n",
      "all of tasks Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-20 17:20:14,350] Trial 0 finished with value: 291.7751262187958 and parameters: {'batch_size': 256, 'dropout': 0.3409096517100847, 'lr': 0.0004231127631425164, 'optimizer': 'Adam'}. Best is trial 0 with value: 291.7751262187958.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "采用DataParallel加速，device_count个数为： 4\n",
      "已完成第： 1 个epoch! Train Loss: 2494.1426713466644 Test Loss: 568.3339703083038\n",
      "已完成第： 2 个epoch! Train Loss: 1795.6325256824493 Test Loss: 347.9686965942383\n",
      "已完成第： 3 个epoch! Train Loss: 1416.697437763214 Test Loss: 345.88529777526855\n",
      "已完成第： 4 个epoch! Train Loss: 1408.2382943630219 Test Loss: 345.1840386390686\n",
      "已完成第： 5 个epoch! Train Loss: 1403.9834320545197 Test Loss: 344.74241495132446\n",
      "已完成第： 6 个epoch! Train Loss: 1400.9367306232452 Test Loss: 344.37344193458557\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "使用study.optimize方法来运行Optuna的超参数搜索过程，设置n_trials参数为搜索的迭代次数。\n",
    "搜索结束后，可以通过study.best_trial获取最佳的超参数组合，打印出最佳超参数以及最小化的损失函数值。\n",
    "\"\"\"\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eef3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_data.shape)\n",
    "print(all_label.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c768c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss,test_loss = np.load(save_loss_path)\n",
    "print(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e124e0",
   "metadata": {},
   "source": [
    "# 5.可视化loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff7cdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 创建 x 轴数据，假设为 epoch 数\n",
    "epoch = np.arange(1, len(train_loss) + 1)\n",
    "\n",
    "train_loss = [round(int(a)/(all_data.shape[0]*0.8), 5) for a in train_loss]\n",
    "print(train_loss)\n",
    "# 绘制训练损失曲线\n",
    "plt.plot(epoch, train_loss, 'b', label='Train Loss')\n",
    "\n",
    "# 关闭科学计数法\n",
    "plt.gca().get_yaxis().get_major_formatter().set_scientific(False)\n",
    "\n",
    "# 设置y轴范围下限为0\n",
    "# plt.ylim(bottom=0)\n",
    "\n",
    "# 设置图表标题和轴标签\n",
    "plt.title('Train Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# 添加图例\n",
    "plt.legend()\n",
    "\n",
    "# 显示图表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ebd587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 x 轴数据，假设为 epoch 数\n",
    "epoch = np.arange(1, len(test_loss) + 1)\n",
    "print(test_loss)\n",
    "test_loss = [round(int(a)/(all_data.shape[0]*0.2), 5) for a in test_loss]\n",
    "\n",
    "\n",
    "# 绘制测试损失曲线\n",
    "plt.plot(epoch, test_loss, 'r', label='Test Loss')\n",
    "\n",
    "# 设置图表标题和轴标签\n",
    "plt.title('Test Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# 添加图例\n",
    "plt.legend()\n",
    "\n",
    "# 显示图表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bec1e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# # 将模型移动到适当的设备\n",
    "# model = model.to(device)\n",
    "\n",
    "# # 打印模型的概要信息\n",
    "# summary(model,input_size=(2, 64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ce42f0",
   "metadata": {},
   "source": [
    "## 6.预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c8b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载模型\n",
    "model = torch.load(save_model_path, map_location=device)# from torchsummary import summary\n",
    "# # 将模型移动到适当的设备\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546182fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 输入数据进行预测\n",
    "input_data =all_data[-10]  # 你的输入数据\n",
    "\n",
    "\n",
    "#调整输入input的维度顺序,作为E，用于下面change_Label_to_a中(E-A)/(B-A)得到a值\n",
    "matrix_64 = input_data.cpu()\n",
    "matrix_64 = matrix_64.permute(1,2,0)  \n",
    "print(matrix_64.shape)\n",
    "\n",
    "\n",
    "# 转换为四维\n",
    "input_data = input_data.unsqueeze(0)#用实际数据，数据格式为(1,2, 64, 64)，不能为2x64x64\n",
    "# input_data = torch.randn(1,2, 64, 64)\n",
    "print(input_data.shape)\n",
    "input_tensor = input_data.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)##如果报错的话需要把网络的设计加上，里面涉及model(input)\n",
    "\n",
    "# 打印预测结果\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830e32fe",
   "metadata": {},
   "source": [
    "# 8.转换格式(label转为最终的weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d94569",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将label面心值转为a值\n",
    "def change_Label_to_a(all_vertical_edge_centers,all_horizontal_edge_centers):\n",
    "    a_vertical = np.zeros((64, 65, 2))\n",
    "    # a_vertical = np.random((64, 65, 2))\n",
    "    a_horizontal = np.zeros((64, 65, 2))\n",
    "    # a_horizontal = np.random.random((64, 65, 2))\n",
    "\n",
    "\n",
    "    # 21. 求a:   横着的边，分两种情况，边缘（对称的）和非边缘的边.这里matrix_64要行列互换，因为横着时面心值是一列一列求得，竖着时是一行一行求的。\n",
    "    for i in range(64):\n",
    "        for j in range(65):\n",
    "            if j == 0 or j == 64:\n",
    "                # a_horizontal[i, j] = (all_horizontal_edge_centers[i, 0] - matrix_64[i, 0]) / (matrix_64[i, 63] - matrix_64[i, 0])#换之前\n",
    "                a_horizontal[i, j] = (all_horizontal_edge_centers[i, 0] - matrix_64[0, i]) / (\n",
    "                    matrix_64[63, i] - matrix_64[0, i])\n",
    "            else:\n",
    "                # aA=(1-a)B=E   a = (E-B)/(A-B) 其中：A为matrix_64[i,j]，B为matrix_64[i,j+1]\n",
    "                # a_horizontal[i, j] = (all_horizontal_edge_centers[i, j] - matrix_64[i, j]) / (matrix_64[i, j-1] - matrix_64[i, j])\n",
    "                a_horizontal[i, j] = (all_horizontal_edge_centers[i, j] - matrix_64[j, i]) / (\n",
    "                    matrix_64[j - 1, i] - matrix_64[j, i])\n",
    "\n",
    "    # 22. 求a:   竖着的边，分两种情况，边缘（对称的）和非边缘的边\n",
    "    for i in range(64):\n",
    "        for j in range(65):\n",
    "            if j == 0 or j == 64:\n",
    "                a_vertical[i, j] = (all_vertical_edge_centers[i, 0] - matrix_64[i, 0]) / (\n",
    "                    matrix_64[i, 63] - matrix_64[i, 0])\n",
    "            else:\n",
    "                # aA=(1-a)B=E   a = (E-B)/(A-B) 其中：A为matrix_64[i,j]，B为matrix_64[i,j+1]\n",
    "                # 2.错误matrix_64[i, j-1]) / (改成matrix_64[i, j]) / (\n",
    "                a_vertical[i, j] = (all_vertical_edge_centers[i, j] - matrix_64[i, j]) / (\n",
    "                    matrix_64[i, j - 1] - matrix_64[i, j])\n",
    "\n",
    "    # # 若最终a对应的矩阵里面出现无穷，则将其替换为0.5.解决了分母为0的问题\n",
    "    # a_vertical[np.isinf(a_vertical)] = 0.5\n",
    "    # a_horizontal[np.isinf(a_horizontal)] = 0.5\n",
    "    #这里64x65x2截成64x64x2,因为边框对称时值相同\n",
    "    \n",
    "    a_vertical = torch.tensor(a_vertical[:,:64,:])\n",
    "    a_horizontal = torch.tensor(a_horizontal[:,:64,:])\n",
    "\n",
    "    print(a_vertical.shape)\n",
    "    print(a_horizontal.shape)\n",
    "    return  a_vertical,a_horizontal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc96529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#返回一个列表，里面嵌套两个子列表，第一个子列表存放的是内部的a值，第二个子列表存放的是边框的a值，\n",
    "#且顺序为上（左到右），下（左到右），左（下到上），右（下到上）\n",
    "def conversion_format(output):\n",
    "    # 调整output的维度顺序\n",
    "    output = output.permute(0,2,3,1)  \n",
    "    print(output.shape)\n",
    "    output = output[0]\n",
    "    print(output.shape)\n",
    "    # print(output)\n",
    "\n",
    "    #将输出output拆成两个面心值\n",
    "    all_vertical_edge_centers = output[:, :, 0:2].cpu()\n",
    "    all_horizontal_edge_centers = output[:, :, 2:4].cpu()\n",
    "    print(all_vertical_edge_centers.shape)\n",
    "    print(all_horizontal_edge_centers.shape)\n",
    "\n",
    "    \n",
    "    a_vertical,a_horizontal  = change_Label_to_a(all_vertical_edge_centers,all_horizontal_edge_centers)    \n",
    "    \n",
    "    # 将前两个元素相加除以二得到一个元素(x+y/)2\n",
    "    avg_vertical = (a_vertical[:, :, 0] + a_vertical[:, :, 1]) / 2\n",
    "    # 将后两个元素相加除以二得到另一个元素\n",
    "    avg_horizontal = (a_horizontal[:, :, 0] + a_horizontal[:, :, 1]) / 2\n",
    "\n",
    "    # 重新组合成新的形状为(64, 64, 2)的张量\n",
    "    new_avg_a_output = torch.stack([avg_vertical, avg_horizontal], dim=2)\n",
    "    # 打印转换后的数据形状\n",
    "    print(new_avg_a_output.shape)\n",
    "\n",
    "    #返回两个求完平均的面心值,包括两个64*64矩阵，矩阵是求完平均后的a值，一个竖着的，一个横着的\n",
    "    vertical_1d = new_avg_a_output[:, :, 0]\n",
    "    horizontal_1d = new_avg_a_output[:, :,1]\n",
    "    print(vertical_1d.shape)\n",
    "    print(horizontal_1d.shape)\n",
    "    \n",
    "    \n",
    "    border = []#存所有边框，四个边框\n",
    "    left_border=[]#存左边框\n",
    "    bottom_border = []#存下边框\n",
    "    inner = []#存内部的面心值\n",
    "\n",
    "    #下面将2个64x64面心值变换格式，返回指定的格式result\n",
    "    for i in range(len(vertical_1d)): #两个for循环等价于for i in range(64):\n",
    "        for j in range(len(vertical_1d[i])):\n",
    "            if j ==0:#j=0添加边框\n",
    "                #添加左边框\n",
    "                left_border.append(vertical_1d[i][0])\n",
    "                #添加下边框\n",
    "                bottom_border.append(horizontal_1d[i][0])\n",
    "            else: \n",
    "                if i !=  63:#当竖着的最后一行时，上面没有对应的横着的\n",
    "                    inner.append(vertical_1d[i][j])#竖着的\n",
    "    #                 print(\"{j-1},{i+1}\",j-1,i+1)\n",
    "                    inner.append(horizontal_1d[j-1][i+1])   #再横着的   \n",
    "                    if j == 63:#如果j=63的话，还需要再加入最后一列的横着的边\n",
    "                        inner.append(horizontal_1d[63][i+1])#当i=63,横着的加最后一列 \n",
    "                else:#if i ==63 :\n",
    "                    inner.append(vertical_1d[63][j])#当i=63,inner最后添加竖着的一行竖线\n",
    "\n",
    "    inner = [tensor.cpu().numpy().tolist() for tensor in inner]#将一维列表里面的tensor元素转为numpy格式，并返回cpu版本\n",
    "    # print(inner)\n",
    "    four_border = [bottom_border,bottom_border,left_border,left_border]#顺序是上（左到右），下（左到右），左（下到上），右（下到上）\n",
    "    border = [item.numpy().tolist() for sublist in four_border for item in sublist]\n",
    "    result = [inner,border]\n",
    "    print(len(result))\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5227f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "result  = conversion_format(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fa4c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(result[0]))\n",
    "print(len(result[1]))\n",
    "print(8064+64*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce8e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = sum(1 for num in result[0] if 0 < num < 1)\n",
    "print(count)\n",
    "print(count/len(result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b1d97b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9b6dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b244dce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9397acc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
