{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c63c957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fb2332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置路径和参数\n",
    "data_path = './data/'\n",
    "model_path = './model/'\n",
    "loss_path = './data/lossa/'\n",
    "\n",
    "num_output_channels = 4\n",
    "dropout = 0.18644844084215567\n",
    "layers = 16\n",
    "lr = 1.3619761379362602e-05\n",
    "batch_size = 2048\n",
    "\n",
    "model_name =  'model_300_optuna_trainloss_326_layer16_batch2048_03.pth'\n",
    "loss_name =  'loss_model_300_optuna_trainloss_326_layer16_batch2048_03.npy'\n",
    "tmp_model_name = 'model_layer16_tmp_50_batch2048_01.pth'\n",
    "tmp_loss_name = 'loss_model_layer16_tmp_50_batch2048_01.npy'\n",
    "\n",
    "# 直接加载npy文件为numpy格式\n",
    "all_data = np.load(data_path + 'all_data.npy')\n",
    "all_label = np.load(data_path + 'all_centerFace_label.npy')\n",
    "\n",
    "all_data = torch.tensor(all_data).float()\n",
    "all_label = torch.tensor(all_label).float()\n",
    "\n",
    "# print(all_data.shape)\n",
    "# print(all_label.shape)\n",
    "\n",
    "\n",
    "#神经网络模型\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_output_channels, dropout, layers, hidden_units):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()  # 用于存储每个层的列表\n",
    "\n",
    "        # 添加卷积层、激活函数、dropout层\n",
    "        for i in range(layers):\n",
    "            if i == 0:\n",
    "                self.layers.append(nn.Conv2d(2, hidden_units, kernel_size=3, stride=1, padding=1))\n",
    "            else:\n",
    "                self.layers.append(nn.Conv2d(hidden_units, hidden_units, kernel_size=3, stride=1, padding=1))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(p=dropout))\n",
    "\n",
    "        # 添加最后一层卷积层\n",
    "        self.conv_final = nn.Conv2d(hidden_units, num_output_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 运行每个层的forward方法\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.conv_final(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "524d5d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "采用DataParallel加速，device_count个数为： 4\n",
      "begin to train!!!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/user/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/user/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/tmp/ipykernel_2193968/2197559904.py\", line 46, in forward\n    x = layer(x)\n  File \"/home/user/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/user/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py\", line 102, in forward\n    return F.relu(input, inplace=self.inplace)\n  File \"/home/user/.local/lib/python3.8/site-packages/torch/nn/functional.py\", line 1298, in relu\n    result = torch.relu(input)\nRuntimeError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 47.54 GiB total capacity; 24.01 GiB already allocated; 376.56 MiB free; 25.12 GiB reserved in total by PyTorch)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 103\u001b[0m\n\u001b[1;32m     99\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(net, model_path \u001b[38;5;241m+\u001b[39mmodel_name)\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_loss, test_loss\n\u001b[0;32m--> 103\u001b[0m train_loss, test_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_output_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_loss)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_loss)\n",
      "Cell \u001b[0;32mIn[5], line 68\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_output_channels, dropout, layers, lr, batch_size, model_name, loss_name)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(testloader):\n\u001b[1;32m     67\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 68\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     70\u001b[0m     running_test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:168\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    167\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 168\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:178\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:86\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     84\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m---> 86\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_utils.py:425\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexc_type, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;66;03m# Some exceptions have first argument as non-str but explicitly\u001b[39;00m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;66;03m# have message field\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexc_type(message\u001b[38;5;241m=\u001b[39mmsg)\n\u001b[0;32m--> 425\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexc_type(msg)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/user/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/user/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/tmp/ipykernel_2193968/2197559904.py\", line 46, in forward\n    x = layer(x)\n  File \"/home/user/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/user/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py\", line 102, in forward\n    return F.relu(input, inplace=self.inplace)\n  File \"/home/user/.local/lib/python3.8/site-packages/torch/nn/functional.py\", line 1298, in relu\n    result = torch.relu(input)\nRuntimeError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 47.54 GiB total capacity; 24.01 GiB already allocated; 376.56 MiB free; 25.12 GiB reserved in total by PyTorch)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train(num_output_channels, dropout, layers, lr, batch_size, model_name, loss_name):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 生成数据集\n",
    "    x_train, x_test, y_train, y_test = train_test_split(all_data, all_label, test_size=0.2)\n",
    "\n",
    "    # 设置种子数\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # 划分数据集\n",
    "    trainset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 划分数据集\n",
    "    testset = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 创建模型实例，并将模型移动到GPU设备上进行计算\n",
    "    net = Net(num_output_channels, dropout, layers, 64).to(device)\n",
    "    \n",
    "    # 加速训练：如果有多个GPU，则使用DataParallel模块\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        net = nn.DataParallel(net)\n",
    "        print(\"采用DataParallel加速，device_count个数为：\", str(torch.cuda.device_count()))\n",
    "        \n",
    "    # 定义损失函数为均方误差\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # 定义优化器为Adam优化器，设置学习率\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    print('begin to train!!!')\n",
    "\n",
    "    # 训练模型\n",
    "    num_epochs = 300  # 训练轮数\n",
    "    \n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    best_loss = float('inf')  # 初始化最佳验证集损失值为正无穷大\n",
    "    patience = 18  # 设置连续多少次验证集损失值不下降时停止训练\n",
    "    count = 0  # 记录连续不下降次数\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        #训练\n",
    "        net.train()\n",
    "        running_train_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(trainloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "            \n",
    "        train_loss.append(running_train_loss)\n",
    "        \n",
    "        # 测试\n",
    "        net.eval()\n",
    "        running_test_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(testloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_test_loss += loss.item()\n",
    "            \n",
    "        test_loss.append(running_test_loss)\n",
    "        print(\"已完成第：\", str(epoch+1), \"个epoch! Train Loss:\", running_train_loss, \"Test Loss:\", running_test_loss)\n",
    "\n",
    "        # 早停法\n",
    "        \"\"\"\n",
    "        如果<连续多个 epoch> 的<验证集损失值>都没有<下降>，即验证集损失值不再降低，那么就会认为模型已经过拟合或者无法继续改善。\n",
    "        这时，训练会提前停止，并保存当前的模型参数\n",
    "        \"\"\"\n",
    "        if running_test_loss < best_loss:\n",
    "            best_loss = running_test_loss\n",
    "            count = 0 #连续十次测试集的epoch loss不下降，故只要又一次下降，就清零重新计算\n",
    "        else:\n",
    "            count += 1\n",
    "            if count >= patience:\n",
    "                print(f\"验证集损失值连续{patience}次不下降，停止训练！\")\n",
    "                break\n",
    "                \n",
    "        if epoch == 50:\n",
    "            torch.save(net, model_path + tmp_model_name)\n",
    "            np.save(loss_path + tmp_loss_name, np.array([train_loss, test_loss]))\n",
    "\n",
    "    end_time = time.time()\n",
    "    process_time = end_time - start_time\n",
    "    print(f\"模型训练和测试共用了: {process_time} 秒！\")\n",
    "    print('all of tasks Finished')\n",
    "    \n",
    "    # 保存整个模型\n",
    "    torch.save(net, model_path +model_name)\n",
    "    #保存损失函数\n",
    "    np.save(loss_path +loss_name, np.array([train_loss, test_loss]))\n",
    "    print(train_loss)\n",
    "    print(test_loss)\n",
    "    return train_loss, test_loss\n",
    "\n",
    "#开始训练\n",
    "train(num_output_channels, dropout, layers, lr, batch_size, model_name, loss_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d88c56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e689ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8470fab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a71343",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
