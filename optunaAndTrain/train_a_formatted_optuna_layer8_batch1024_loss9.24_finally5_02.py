import torch
import torch.nn as nn
import torch.optim as optim
import re
import numpy as np
from sklearn.model_selection import train_test_split
import time
from sklearn.model_selection import train_test_split

np.set_printoptions(threshold=np.inf)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
"""
å·²å®Œæˆç¬¬ï¼š 1 ä¸ªepoch! Train Loss: 7.887098919600248 Test Loss: 1.6204932369291782
å·²å®Œæˆç¬¬ï¼š 2 ä¸ªepoch! Train Loss: 6.430324245244265 Test Loss: 1.6020091660320759
å·²å®Œæˆç¬¬ï¼š 3 ä¸ªepoch! Train Loss: 6.314974147826433 Test Loss: 1.5438192784786224
å·²å®Œæˆç¬¬ï¼š 4 ä¸ªepoch! Train Loss: 6.026678558439016 Test Loss: 1.407630294561386
å·²å®Œæˆç¬¬ï¼š 5 ä¸ªepoch! Train Loss: 5.543837275356054 Test Loss: 1.264107447117567
å·²å®Œæˆç¬¬ï¼š 6 ä¸ªepoch! Train Loss: 5.146136976778507 Test Loss: 1.1853917501866817
æ¨¡å‹è®­ç»ƒå’Œæµ‹è¯•å…±ç”¨äº†: 613.6281049251556 ç§’ï¼
all of tasks Finished
[32m[I 2023-07-28 23:55:49,607][0m Trial 31 finished with value: 5.146136976778507 and parameters: {'batch_size': 1024, 'dropout': 0.1309991291382774, 'layers': 8, 'lr': 0.002506915874027759, 'optimizer': 'Adam'}. Best is trial 31 with value: 5.146136976778507.
"""
# è®¾ç½®è·¯å¾„å’Œå‚æ•°
data_path = '../data/'
model_path = './model/'
loss_path = './loss/'

num_output_channels = 4
dropout = 0.1309991291382774
layers = 8
lr = 0.002506915874027759
batch_size = 1024

model_name =  'model_300_layer8_optuna_final_03.pth'
loss_name =  'loss_model_300_layer8_optuna_final_03.npy'
tmp_model_name = 'tmp_model_final_200_01.pth'
tmp_loss_name = 'tmp_loss_model_final_200_01.npy'

# ç›´æ¥åŠ è½½npyæ–‡ä»¶ä¸ºnumpyæ ¼å¼
all_data = np.load(data_path + 'all_data.npy')
all_label = np.load(data_path + 'all_label_repair01.npy')#ä¿®æ”¹åçš„all_label_repair01.npy'

#æ•°æ®é›†åˆ‡ç‰‡ï¼Œå‡å°è®­ç»ƒæ—¶é—´
# slice = 10000
all_data = torch.tensor(all_data).float()
all_label = torch.tensor(all_label).float()

# print(all_data.shape)
# print(all_label.shape)


#ç¥ç»ç½‘ç»œæ¨¡å‹
class Net(nn.Module):
    def __init__(self, num_output_channels, dropout, layers, hidden_units):
        super(Net, self).__init__()

        self.layers = nn.ModuleList()  # ç”¨äºå­˜å‚¨æ¯ä¸ªå±‚çš„åˆ—è¡¨

        # æ·»åŠ å·ç§¯å±‚ã€æ¿€æ´»å‡½æ•°ã€dropoutå±‚
        for i in range(layers):
            if i == 0:
                self.layers.append(nn.Conv2d(2, hidden_units, kernel_size=3, stride=1, padding=1))
            else:
                self.layers.append(nn.Conv2d(hidden_units, hidden_units, kernel_size=3, stride=1, padding=1))
            self.layers.append(nn.ReLU())
            self.layers.append(nn.Dropout(p=dropout))

        # æ·»åŠ æœ€åä¸€å±‚å·ç§¯å±‚
        self.conv_final = nn.Conv2d(hidden_units, num_output_channels, kernel_size=3, stride=1, padding=1)

    def forward(self, x):
        # è¿è¡Œæ¯ä¸ªå±‚çš„forwardæ–¹æ³•
        for layer in self.layers:
            x = layer(x)

        x = self.conv_final(x)

        return x
    


def train(num_output_channels, dropout, layers, lr, batch_size, model_name, loss_name):
    start_time = time.time()

    # ç”Ÿæˆæ•°æ®é›†
    x_train, x_test, y_train, y_test = train_test_split(all_data, all_label, test_size=0.2)

    # è®¾ç½®ç§å­æ•°
    seed = 42
    torch.manual_seed(seed)

    # åˆ’åˆ†æ•°æ®é›†
    trainset = torch.utils.data.TensorDataset(x_train, y_train)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)

    # åˆ’åˆ†æ•°æ®é›†
    testset = torch.utils.data.TensorDataset(x_test, y_test)
    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)

    # åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼Œå¹¶å°†æ¨¡å‹ç§»åŠ¨åˆ°GPUè®¾å¤‡ä¸Šè¿›è¡Œè®¡ç®—
    net = Net(num_output_channels, dropout, layers, 64).to(device)
    
    # åŠ é€Ÿè®­ç»ƒï¼šå¦‚æœæœ‰å¤šä¸ªGPUï¼Œåˆ™ä½¿ç”¨DataParallelæ¨¡å—
    if torch.cuda.device_count() > 1:
        net = nn.DataParallel(net)
        print("é‡‡ç”¨DataParallelåŠ é€Ÿï¼Œdevice_countä¸ªæ•°ä¸ºï¼š", str(torch.cuda.device_count()))
        
    # å®šä¹‰æŸå¤±å‡½æ•°ä¸ºå‡æ–¹è¯¯å·®
    criterion = nn.MSELoss()
    
    # å®šä¹‰ä¼˜åŒ–å™¨ä¸ºAdamä¼˜åŒ–å™¨ï¼Œè®¾ç½®å­¦ä¹ ç‡
    optimizer = optim.Adam(net.parameters(), lr=lr)

    print('begin to train!!!')

    # è®­ç»ƒæ¨¡å‹
    num_epochs = 300  # è®­ç»ƒè½®æ•°
    
    train_loss = []
    test_loss = []
    best_loss = float('inf')  # åˆå§‹åŒ–æœ€ä½³éªŒè¯é›†æŸå¤±å€¼ä¸ºæ­£æ— ç©·å¤§
    patience = 300  # è®¾ç½®è¿ç»­å¤šå°‘æ¬¡éªŒè¯é›†æŸå¤±å€¼ä¸ä¸‹é™æ—¶åœæ­¢è®­ç»ƒ
    count = 0  # è®°å½•è¿ç»­ä¸ä¸‹é™æ¬¡æ•°
    
    for epoch in range(num_epochs):
        
        #è®­ç»ƒ
        net.train()
        running_train_loss = 0.0
        for i, (inputs, labels) in enumerate(trainloader):# trainloaderæ˜¯æ‰€æœ‰æ•°æ®åŒ…æ‹¬iä¸ªbatchç»„
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()

            outputs = net(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_train_loss += loss.item()
            
        train_loss.append(running_train_loss)
        
        # æµ‹è¯•
        net.eval()
        running_test_loss = 0.0
        for i, (inputs, labels) in enumerate(testloader):
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = net(inputs)
            loss = criterion(outputs, labels)
            running_test_loss += loss.item()
            
        test_loss.append(running_test_loss)
        
        
        print("å·²å®Œæˆç¬¬ï¼š", str(epoch+1), "ä¸ªepoch! Train Loss:", running_train_loss, "Test Loss:", running_test_loss)

        # æ—©åœæ³•
        """
        å¦‚æœ<è¿ç»­å¤šä¸ª epoch> çš„<éªŒè¯é›†æŸå¤±å€¼>éƒ½æ²¡æœ‰<ä¸‹é™>ï¼Œå³éªŒè¯é›†æŸå¤±å€¼ä¸å†é™ä½ï¼Œé‚£ä¹ˆå°±ä¼šè®¤ä¸ºæ¨¡å‹å·²ç»è¿‡æ‹Ÿåˆæˆ–è€…æ— æ³•ç»§ç»­æ”¹å–„ã€‚
        è¿™æ—¶ï¼Œè®­ç»ƒä¼šæå‰åœæ­¢ï¼Œå¹¶ä¿å­˜å½“å‰çš„æ¨¡å‹å‚æ•°
        """
        if running_test_loss < best_loss:
            best_loss = running_test_loss
            count = 0 #è¿ç»­åæ¬¡æµ‹è¯•é›†çš„epoch lossä¸ä¸‹é™ï¼Œæ•…åªè¦åˆä¸€æ¬¡ä¸‹é™ï¼Œå°±æ¸…é›¶é‡æ–°è®¡ç®—
        else:
            count += 1
            if count >= patience:
                print(f"éªŒè¯é›†æŸå¤±å€¼è¿ç»­{patience}æ¬¡ä¸ä¸‹é™ï¼Œåœæ­¢è®­ç»ƒï¼")
                break

        #å½“epoch=30æ—¶ä¿å­˜ä¸€æ¬¡modelä¸loss
        if epoch == 99:
            torch.save(net, model_path + tmp_model_name)
            np.save(loss_path + tmp_loss_name, np.array([train_loss, test_loss]))

    end_time = time.time()
    process_time = end_time - start_time
    print(f"æ¨¡å‹è®­ç»ƒå’Œæµ‹è¯•å…±ç”¨äº†: {process_time} ç§’ï¼")
    print('all of tasks Finished')
    
    # ä¿å­˜æ•´ä¸ªæ¨¡å‹
    torch.save(net, model_path +model_name)
    #ä¿å­˜æŸå¤±å‡½æ•°
    np.save(loss_path +loss_name, np.array([train_loss, test_loss]))
    print(train_loss)
    print(test_loss)
    return train_loss, test_loss

#å¼€å§‹è®­ç»ƒ
train(num_output_channels, dropout, layers, lr, batch_size, model_name, loss_name)


